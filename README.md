# LLM Finetuning using LoRA and Quantization on Custom Dataset

This project provides an implementation for fineâ€‘tuning large language models (LLMs) using **LoRA (Lowâ€‘Rank Adaptation)** on a custom dataset. LoRA is a parameterâ€‘efficient technique that allows you to adapt powerful pretrained models (like Mistral, LLaMA, or GPTâ€‘style architectures) without retraining the entire network. This makes fineâ€‘tuning faster, cheaper, and more accessible.  

In addition, the project leverages **4â€‘bit quantization (BitsAndBytes)** to reduce memory usage and enable training on consumerâ€‘grade GPUs. By combining quantization with LoRA, the workflow achieves efficient fineâ€‘tuning without sacrificing model performance, making largeâ€‘scale adaptation practical even on limited hardware.  

A key part of this project is the **automatic dataset generation pipeline**: PDFs are converted into text, then an LLM is used to generate a Q&A dataset that will be used to finetune the model. This produces a highâ€‘quality, domainâ€‘specific dataset for instruction tuning without requiring manual annotation.

---

## ğŸš€ Features

- Fineâ€‘tune stateâ€‘ofâ€‘theâ€‘art LLMs with **LoRA adapters**
- Automatically generate a **Q&A dataset** from PDFs using an LLM
- Train on your own custom dataset (e.g., Q&A pairs, domainâ€‘specific text)
- Leverage **4â€‘bit quantization (BitsAndBytes)** to reduce memory usage and enable efficient training on consumerâ€‘grade GPUs


---

## ğŸ“‚ Project Structure
- `main.py` â€“ entry point for training and evaluation
- `src/` â€“ PDFtoQA.py, finetuning.py
- `data/` â€“ place your custom dataset files (e.g., `QA.json`)
- `workspace/` â€“ model checkpoints and outputs
- `config.yaml` â€“ configuration file (paths, model IDs, dataset references)
- `params.yaml` â€“ parameters file (training, quantization, LoRA settings)
- `requirements.txt` â€“ requirements file
- `.gitignore` â€“ ensures large files (weights, cache, PDFs) are not pushed to GitHub



### ğŸ“ PDFtoQA.py
- Converts a PDF into text and splits it into smaller chunks  
- Builds a prompt that instructs an LLM to generate Q&A pairs from each chunk  
- Streams responses back from the LLM in real time  
- Cleans and parses the responses into valid JSON with `"question"` and `"answer"` fields  
- Collects all Q&A pairs and saves them into a JSON file for later use  


### ğŸ§  finetuning.py
- **Dataset preparation**  
  - Loads a dataset containing `question` and `answer` fields  
  - Formats each entry into a chatâ€‘style template (system, user, assistant roles)  
  - Produces text samples suitable for instructionâ€‘tuning  

- **Tokenizer setup**  
  - Loads the tokenizer for the chosen base model  
  - Applies the custom chat template to dataset entries  

- **Model setup with quantization**  
  - Loads the base language model with 4â€‘bit quantization (`BitsAndBytesConfig`) to save memory  
  - Enables gradient checkpointing for efficiency  
  - Prepares the model for lowâ€‘bit training  

- **LoRA configuration**  
  - Defines LoRA adapter parameters (rank, alpha, dropout, target modules, task type)  
  - Wraps the model with LoRA for parameterâ€‘efficient fineâ€‘tuning  

- **Trainer setup and training**  
  - Uses `SFTTrainer` (Supervised Fineâ€‘Tuning) from TRL to train the model  
  - Configures training (epochs, logging, disables checkpoint saving)  
  - Runs the training loop on the prepared dataset  

- **Model saving and publishing**  
  - Saves the trained model locally (`complete_checkpoint`, `final_model`)  
  - Optionally pushes the model and tokenizer to Hugging Face Hub if a `repo_id` is 



---


## âš™ï¸ Installation & Setup

### âš™ï¸ Environment
- **Create  and activate environment**
  ```bash
    conda create -p venv python==3.13 -y
    conda activate venv
    ```
### âš™ï¸ Requirements
- **Install dependencies**
  ```bash
    pip install -r requirements.txt
    ```


### ğŸ‹  Ollama
- **Download and install Ollama**
  ```bash
    curl -fsSL https://ollama.com/install.sh | sh -- Linux
    ```
  ```bash
    brew install ollama -- Mac
    ```
- **Run Ollama**
  ```bash
    ollama serve -- keeps Ollama running
    ```
- **Download model**
  ```bash
    ollama pull qwen2.5:3b
    ```
### ğŸ¤ Hugging Face Hub
- **Install the HuggingFace hub client**
  ```bash
    pip install huggingface_hub
    ```
- **Login with your token (must be fineâ€‘grained and enable gated repo access):**
  ```bash
    huggingface-cli login
    ```
- **Enter your huggingface token when prompted.**

## â–¶ï¸ Run the Project
To launch the application, simply execute:
```bash
    python main.py
```

## âœ¨ Author

Developed by Antonio Maria Fiscarelli This repository is a personal project exploring efficient fineâ€‘tuning of LLMs.